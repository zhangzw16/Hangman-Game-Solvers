dataset: 'dataset/' # dataset directory
models: 'models/' # for storing models
plots: 'plots/' # for plots
pickle: 'pickle/' # pickle dumps root path

cuda: True #whether to use NVIDIA cuda
    
test_per_epoch: 0 #test per epoch i.e. how many times in ONE epoch
test_every_epoch: 200 #after how many epochs
print_per_epoch: 3 #print loss function how often in each epoch
save_every: 5 #save models after how many epochs
plot_every: 5 #save plots for test loss/train loss/accuracy

resume: True #resume training from saved model

lr: 0.0005 #learning rate

drop_uniform: False #whether dropping of character sets is independent of set size
reset_after: 1 #generate a new random dataset after these manh epochs
vocab_size: 26 #size of vocabulary. 26 engliush letters in our case
min_len: 3 #words with length less than min_len are not added to the dataset

rnn: 'GRU' #type of RNN. Can be LSTM/GRU
use_embedding: True #whether to use character embeddings
embedding_dim: 256 #if use_embedding, dimension of embedding
hidden_dim: 1024 #hidden dimension of RNN
output_mid_features: 512 #number of neurons in hidden layer after RNN
miss_linear_dim: 512 #miss chars are projected to this dimension using a simple linear layer
num_layers: 4 #number of layers in RNN
dropout: 0.3 #dropout
batch_size: 2048 #batch size for training and testing
epochs: 200 #total no. of epochs to train
num_workers: 64